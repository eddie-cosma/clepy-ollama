{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25895ddc-7d80-49b1-83cf-3df48482a4c0",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Let's start by importing the Ollama library. This library is modeled after the Ollama web API, making it easy to interact with Ollama through Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b28cb41-1960-410a-86b0-824287ee5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7565c84-dbc2-46e9-aea1-4579bc104813",
   "metadata": {},
   "source": [
    "## Make sure Ollama is running\n",
    "\n",
    "Go to `http://localhost:11434` in your browser and make sure you get the response `Ollama is running`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36323679-353c-4515-ba69-60ba648e0d4c",
   "metadata": {},
   "source": [
    "## Pulling a model\n",
    "\n",
    "We already pulled our model earlier, so this step is not necessary. It is included here for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec76ee99-f7b5-4be3-9671-c6407f2dbc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ollama.pull('tinyllama')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d5093-289d-4a85-b9c7-b080c7b55ca6",
   "metadata": {},
   "source": [
    "## List available models\n",
    "\n",
    "If you have other models installed, you can list them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6455f235-dfb2-4145-b699-543d2850cfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='tinyllama:latest', modified_at=datetime.datetime(2025, 3, 7, 17, 50, 22, 405498, tzinfo=TzInfo(-06:00)), digest='2644915ede352ea7bdfaff0bfac0be74c719d5d5202acb63a6fb095b52f394a4', size=637700138, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1B', quantization_level='Q4_0')), Model(model='deepseek-r1:latest', modified_at=datetime.datetime(2025, 2, 7, 19, 53, 28, 828200, tzinfo=TzInfo(-06:00)), digest='0a8c266910232fd3291e71e5ba1e058cc5af9d411192cf88b6d30e92b6e73163', size=4683075271, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2025, 2, 7, 19, 42, 22, 49375, tzinfo=TzInfo(-06:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71461b8-95f6-4c83-9ec7-d5274b335933",
   "metadata": {},
   "source": [
    "Well ðŸ¤” that's kind of ugly. The API response is being cast as a couple of nested objects. Let's just get the names of the models from the API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39c8cf1-924b-4a07-a928-ccbd623f5274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tinyllama:latest', 'deepseek-r1:latest', 'llama3.2:latest']\n"
     ]
    }
   ],
   "source": [
    "response = ollama.list()\n",
    "model_names = [response_model.model for response_model in response.models]\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743e175-1fef-4d6f-bf22-69b0ead01d10",
   "metadata": {},
   "source": [
    "## Start a chat\n",
    "\n",
    "Chats can be performed either all at once or in \"streaming\" mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
